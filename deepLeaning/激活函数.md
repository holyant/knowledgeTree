>sigmoid 可用于只有一个输出单元的二元分类。但是如果进行多项分类的话，则需要使用多个输出单元（每个类别一个单元），并对输出进行 softmax 激活。

##sigmoid
$\frac{1}{1+e^-x)}$

####激活函数
```python
def sigmoid(x):
    # TODO: Implement sigmoid function
    return 1/(1 + np.exp(-x))

```
####激活函数求导
```python
def sigmoid_prime(x):
    return sigmoid(x) * (1 - sigmoid(x))
```

##修正线性单元ReLU
>1. f(x)=max(x,0)
>2. 即如果输入大于 0，则输出等于输入


##softmax
>和 sigmoid 一样，softmax 函数将每个单元的输出压缩到 0 和 1 之间。但 softmax 函数在拆分输出时，会使输出之和等于 1。softmax 函数的输出等于分类概率分布，显示了任何类别为真的概率。

####使用tensorflow计算softmax
```python
# Solution is available in the other "solution.py" tab
import tensorflow as tf


def run():
    output = None
    logit_data = [2.0, 1.0, 0.1]
    logits = tf.placeholder(tf.float32)
    
    # TODO: Calculate the softmax of the logits
    softmax = tf.nn.softmax(logits)  
    
    with tf.Session() as sess:
        # TODO: Feed in the logit data
        # output = sess.run(softmax,    )
        output = sess.run(softmax, feed_dict={logits: logit_data})
    return output

```

####使用tensorflow求交叉熵
```python
import tensorflow as tf

softmax_data = [0.7, 0.2, 0.1]
one_hot_data = [1.0, 0.0, 0.0]

softmax = tf.placeholder(tf.float32)
one_hot = tf.placeholder(tf.float32)

# TODO: Print cross entropy from session
cross_entropy = -tf.reduce_sum(tf.multiply(one_hot, tf.log(softmax)))

with tf.Session() as sess:
    print(sess.run(cross_entropy, feed_dict={softmax: softmax_data, one_hot: one_hot_data}))
```


